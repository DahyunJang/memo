* todo
** tensor flow
*** tf eco system
*** tf lite 조사
*** tf guide reading
** cpp study
*** gof
*** cppunit, google test
** white paper reading
*** dandelion
* Q search keywords
*** lite version compilation
* summary - tf guide
** embadding
- multi dimesion 을 소수의 디멘션으로 줄인 것, 리니어한 pca, 넌 리니어한  t-SNE 방법이 있음.
** tensor
- 다차원 배열이고, 변경가능한 variable, 변경 불가능한 constant, 데이터를 로드하게 되어있는 place holder 등.
- 텐서는 모양과 데이터 타입으로 정의 됨.
- eval을 해야 텐서의 값이 유의미해짐.
** graphs and sessions
- dataflow graph의 일부를 실행시키는 것을 session이라고 함.
- node 는 연산, edge는 input에 해당함.
- graph structure와 그래프 연산 대상이 되는 메타데이터들을 저장하는 collection으로 구성됨.
- job - 연산할 device 명시 가능. 몇 개의 셋을 할당할 수도 있음.
- operation 정의하고 나면 sess.run으로 실행함.

* summary - tf white paper
** programming model and basic concept
- op : abstract computation
- kernel : 디바이스별 op 구현
- session :

* summary - eco system
** apache mesos
- 클러스터 자원을 하나의 컴퓨터 처럼 사용할 수 있게 해줌
* summary - mapreduce
- 분산 환경에서 'key-value' map 을 'key-value list'로 reduce 하는 과정
- map task를 수행하는 워커들(M)과 reduce task를 수행하는 워커들(R)이 있음
- 맵 후 리듀스 워커에 할당은 해시 기반으로
- R에 따라 아웃풋(R마다 아웃풋 파일 하나)이 묶이는 것을 고려해 해시키를 조절 할 수 있다.
- 마스터가 잡을 스케줄링하고 워커에 rpc 를 호출.
- 대부분의 연산은 deterministic 해서 여러 번 수행되는 것은 문제가 아니다.
- gfs는 세 쌍의 데이터를 저장한다. 마스터은 이 데이터가 있는 혹은 가까이 있는 노드에 태스크 할당.
- 로드 밸런싱 등의 이유로 태스크가 워커보다 많은 게 좋음.
- 맵퍼는 data 가 16-64MB로 쪼개지게 선택.
- 리듀서는 또 너무 많으면 리듀서 수많큼 너무 많은 output file이 나오는 불편한 상황이 생김.
- 네트워크나, 다른 머신 상황으로 인해 수행속도가 엄청나게 느린 staggler들이 가장 큰 성능 저하의 원인
- 중간에 sort를 하는 이유는, 보기 편함 + direct access 가능
- combiner function : 한 맵퍼에서 충분히 리듀스를 할 만큼 pair가 많이 나오면 네트워크 낭비하지 말고 맵퍼에서 하자.
- 맵이나 리듀스가 안되는 잘못된 형식에 대한 데이터는, 마스터가 실패 시그널을 받고 거름.
- 로컬에서 시퀀셜하게 돌리면서 디버깅 가능
- couter: 해당 밸류가 얼마나 있는지를 count. Task 중복에도 제대로 카운팅 될 수 있도록 관리해줌.
* summary - stanford lecture7 - introduction to tf
** node -> op, edge -> tensor, w, b -> variable, x -> palceholder
*** variable (튜닝용)
- stateful nodes witch output their current value
- make gradient update too. tune to minimize the loss
#+BEGIN_SRC python
 b = tf.Variable(tf.zeros((100,)))
 W = tf.Variable(tf.random_uniform((784,100), -1, 1))
#+END_SRC
*** palceholder (state 없는 인풋 데이터)
- fed in execution time. input. assign data and shape.
- x = tf.placeholder(tf.float32, (100, 784))
*** node
- mathmatical
- variable with initial value, but placeholder is not. no data at init.

#+BEGIN_SRC python
 h = tf.nn.relu(tf.matmul(x,W) + b)
#+END_SRC

** session: binding to a particular execution context.
- fetches : list of graph nodes. return the outputs of theses nodes.
- feeds : dictionary mapping from graph nodes to concrete values.
- sess.run(fetches, feeds)
- concept of lazy execution -- > session run 시에 계산함.

#+BEGIN_SRC python
 sess =tf.Session()
 sess.run(tf.initialize_all_variables())
 sess.run(h, {x: np.random.random(100,784)})
#+END_SRC

- x 를 dcitionary 형태로 넘겨서 h 오퍼레이션 실행함.
*** how do we define the loss
- use placeholder ofr labels
- build loss node using labes and prediction.

#+BEGIN_SRC python
 prediction = tf.nn.softmax(..) -> output of neural network  //점수
 label = tf.placeholder(tf.float32, [100,10])
 cross_entropy = -tf.reduce_sum(label * tf.log(prediction), axis=1)
#+END_SRC

*** how do we compute gradients?
- use optizizer object. add optimization operation to computation graph.
- apply gradient to variables.
#+
 train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
#+
*** traing the model
- create session -> build training schedule -> run train_step.
#+
 sess = tf.Session()
 sess.run(tf.initialize_all_variable())

 for i in trange(1000):
  batch_x, batch_lable = data.next_batch()
  sess.run(train_step, feed_dict = { x: batch_x, label: batch_label})
  //train_step안에, loss function 에서 batch_label 사용함.
#+
*** variable sharing
- train model over sevral session.
- variable scope..
- get_variable - access or create in designated var in the scope.
*** summary of running.
- build graph
- initialize session
- run step op.
#+BEGIN_SRC python
// x input, y label.
def generate_dataset():
 x_batch = np.linspace(-1, 1, 101)
 y_batch = 2 * x_batch + np.random.randn(*x_batch.shape) * 0.3
 return x_batch, y_batch

def linear_regression():
 x = tf.placeholder(tf.float32, shape=(NOne,), name='x')
 y = tf.placeholder(tf.float32, shape=(None,), name='y')
 with tf.variable_scope(lreg') as scope:
  w = tf.variable(np.random.normal(), name='W')
  y_pred=tf.mul(w,x)

  loss = tf.reduce_mean(tf.square(y_pred - y ))
retrun x, y, y_pred, loss

def run():
 x_batch, y_batch = generate_dataset()
 x, y, y_pred, loss = liniear_regression()
 optiziser = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
 init = tf.global_variables_initializer()

 with tf.Session() as session:
  session.run(init)

  feed_dict = {x: x_batch, y: y_batch}
  for _ in range(30):
   loss_val, _ = session. run([loss, optimizer], feed_dict)
   print('loss:', loss_val,mean())

  y_pred_batch = session.run(y_pred, {x:x_batch})

 plt.figure(1)
 plt.scatter(x_batch, y_batch)
 plt.plot(x_batch, y_pred_batch)


#+END_SRC
